{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH4T0peGmF-Y"
      },
      "source": [
        "### Developing the LSH method from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "fZX4Yy9Wi3CR"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "def cosine_distance(x, y):\n",
        "    return sum([i*j for i,j in zip(x, y)])/(sqrt(sum([i*i for i in x]))*sqrt(sum([i*i for i in y])))\n",
        "\n",
        "class Image:\n",
        "    def __init__(self, f, d):\n",
        "        self.projections = np.random.randn(f, d)\n",
        "        \n",
        "    def hash_point(self, x):\n",
        "        bools = (np.dot(x, self.projections.T) > 0).astype('int')\n",
        "        return ''.join(bools.astype('str'))\n",
        "\n",
        "class LSHIndex:\n",
        "    def __init__(self, f, b, d):\n",
        "        self.hash_tables = []\n",
        "        self.b = b\n",
        "        self.X = {}\n",
        "        self.image = Image(f, d)\n",
        "        for i in range(self.b):\n",
        "            self.hash_tables.append({})\n",
        "    \n",
        "    def index(self, key, x):\n",
        "        # storing all input vectors + its key in a dictionary to use it later\n",
        "        self.X[key] = x\n",
        "        # hash the input vector into f dimentions\n",
        "        generated_vector = self.image.hash_point(x)\n",
        "        # calculating how many charachters each b segment has\n",
        "        part_len = len(generated_vector)//self.b\n",
        "\n",
        "        for i in range(self.b):\n",
        "            if (i != (self.b - 1)):\n",
        "                binary = generated_vector[i*part_len: (i+1)*part_len]\n",
        "            else:\n",
        "                binary = generated_vector[i*part_len:]\n",
        "\n",
        "            # binary to decimal\n",
        "            decimal = int(binary, 2)\n",
        "            # store each decimal representation corresponding its key in the i'th hash table\n",
        "            self.hash_tables[i][key] = decimal\n",
        "\n",
        "        return self.hash_tables\n",
        "\n",
        "    def query(self, q):\n",
        "        # for storing candidate vectors keys, define a list \n",
        "        candid_vectors_keys = []\n",
        "        # for storing candidate vectors cosine similarity with q, define a list\n",
        "        all_cosines = []\n",
        "        # we again hash q with same method as bedore\n",
        "        generated_vector = self.image.hash_point(q)\n",
        "        part_len = len(generated_vector)//self.b\n",
        "\n",
        "        for i in range(self.b):\n",
        "            if (i != (self.b - 1)):\n",
        "                binary = generated_vector[i*part_len: (i+1)*part_len]\n",
        "            else:\n",
        "                binary = generated_vector[i*part_len:]\n",
        "\n",
        "            decimal = int(binary, 2)\n",
        "\n",
        "            # we find which key(s) have the same decimal representation of hashed value and if there exist such a key, we store that key\n",
        "            candid_vector_key = next((key for key, decimals in self.hash_tables[i].items() if decimals == decimal), None)\n",
        "            if candid_vector_key != None:\n",
        "              if candid_vector_key not in candid_vectors_keys:\n",
        "                candid_vectors_keys.append(candid_vector_key)\n",
        "            \n",
        "        # getting original vectors for the corresponding key values\n",
        "        for key in candid_vectors_keys:\n",
        "            all_cosines.append((key, cosine_distance(self.X.get(key), q)))\n",
        "\n",
        "        all_cosines.sort(key=lambda i:i[1], reverse=True)\n",
        "\n",
        "        results = []\n",
        "        for i in range(10):\n",
        "          if all_cosines[i][1] < 1: # excluding the q vector similarity with its self\n",
        "            results.append(all_cosines[i][0])\n",
        "        \n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC8WJ01FmUGD"
      },
      "source": [
        "### Here we imported libraries + imported a pre-trained gensim model named glove-wiki-gigaword-300 (we use 300 model since we want to vectorize every word into a 300 dimention vector !). In addition, we imported the csv dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0jHG38mK8Vh",
        "outputId": "be608cb5-43c9-4ef9-c576-75853994fe48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "import gensim\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "model = api.load('glove-wiki-gigaword-300')\n",
        "\n",
        "# Mounting dataset from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "with open('/content/gdrive/MyDrive/title.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnrN9dLIm7LP"
      },
      "source": [
        "### for each sentences in the csv dataset, given that we have the vectorized representation of words using pre-trained model, we calculated avg of words' vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Op_4p99DaIan"
      },
      "outputs": [],
      "source": [
        "sentence_vectors = []\n",
        "for sentence in data:\n",
        "  vector = [0]*300\n",
        "  word_number = 0\n",
        "  for word in sentence[1].split():\n",
        "    try:\n",
        "      word_number += 1\n",
        "      vector = np.add(model[word], vector)\n",
        "    except:\n",
        "      continue\n",
        "  \n",
        "  sentence_vectors.append(np.divide(vector, word_number))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "bzSlxZx7dc8F"
      },
      "outputs": [],
      "source": [
        "target_sentence_1 = 'end of world'\n",
        "vector = [0]*300\n",
        "for word in target_sentence_1.split():\n",
        "  vector = np.add(model[word], vector)\n",
        "\n",
        "target_sentence_1_vector = np.divide(vector, 3)\n",
        "\n",
        "target_sentence_2 = 'he and his friends'\n",
        "vector = [0]*300\n",
        "for word in target_sentence_2.split():\n",
        "  vector = np.add(model[word], vector)\n",
        "\n",
        "target_sentence_2_vector = np.divide(vector, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIa8lvTongF4"
      },
      "source": [
        "### We first indexed every senteces into hash tables using LSH.index, then we made query (our query here is the target senteces). Here we printed out IDs of sentences that are similar to the target sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwj0HNhMjQWJ",
        "outputId": "76afe225-89f2-4d00-b6ae-3775069e184b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similar text to \"end of world\" by ID :  [79, 132, 445, 16, 86, 143, 9978, 19, 12]\n",
            "similar text to \"he and his friends\" by ID :  [285, 79, 253, 744, 16, 39, 93, 143, 24, 12]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "N = len(sentence_vectors)\n",
        "d = 300\n",
        "f = 100\n",
        "b = 20\n",
        "r = 5\n",
        "k = 10\n",
        "\n",
        "LSH = LSHIndex(f, b, d)\n",
        "for i in range(N):\n",
        "  LSH.index(i, sentence_vectors[i])\n",
        "\n",
        "print('similar text to \"end of world\" by ID : ', LSH.query(target_sentence_1_vector))\n",
        "print('similar text to \"he and his friends\" by ID : ', LSH.query(target_sentence_2_vector))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW3_question_4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
